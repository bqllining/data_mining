{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37364bitbaseconda7fc16dbd5b7d4d5796333912991fe30d",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简单加权平均，结果直接融合\n",
    "# 生成一些简单的样本数据，test_prei代表第i个模型的预测值\n",
    "test_pre1 = [1.2, 3.2, 2.1, 6.2]\n",
    "test_pre2 = [0.9, 3.1, 2.0, 5.9]\n",
    "test_pre3 = [1.1, 2.9, 2.2, 6.0]\n",
    "\n",
    "# y_test_true代表模型的真实值\n",
    "y_test_true = [1, 3, 2, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 定义结果的加权平均函数\n",
    "def Weighted_method(test_pre1, test_pre2, test_pre3, w=[1/3,1/3,1/3]):\n",
    "    Weighted_result = w[0]*pd.Series(test_pre1)+w[1]*pd.Series(test_pre2)+w[2]*pd.Series(test_pre3)\n",
    "    return Weighted_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Pred1 MAE: 0.1750000000000001\nPred2 MAE: 0.07499999999999993\nPred3 MAE: 0.10000000000000009\n"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "# 各模型的预测结果计算MAE\n",
    "print('Pred1 MAE:', metrics.mean_absolute_error(y_test_true, test_pre1))\n",
    "print('Pred2 MAE:', metrics.mean_absolute_error(y_test_true, test_pre2))\n",
    "print('Pred3 MAE:', metrics.mean_absolute_error(y_test_true, test_pre3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Weighted_pre MAE: 0.05750000000000027\n"
    }
   ],
   "source": [
    "# 根据加权计算MAE\n",
    "w = [0.3, 0.4, 0.3] # 定义比重权值\n",
    "Weighted_pre = Weighted_method(test_pre1, test_pre2, test_pre3, w)\n",
    "print('Weighted_pre MAE:', metrics.mean_absolute_error(y_test_true, Weighted_pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义结果的加权平均函数\n",
    "def Mean_method(test_pre1, test_pre2, test_pre3):\n",
    "    Mean_result = pd.concat([pd.Series(test_pre1), pd.Series(test_pre2), pd.Series(test_pre3)],axis=1).mean(axis=1)\n",
    "    return Mean_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Mean_pre MAE: 0.06666666666666693\n"
    }
   ],
   "source": [
    "Mean_pre = Mean_method(test_pre1, test_pre2, test_pre3)\n",
    "print('Mean_pre MAE:', metrics.mean_absolute_error(y_test_true, Mean_pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义结果的加权平均函数\n",
    "def Median_method(test_pre1,test_pre2,test_pre3):\n",
    "    Median_result = pd.concat([pd.Series(test_pre1),pd.Series(test_pre2),pd.Series(test_pre3)],axis=1).median(axis=1)\n",
    "    return Median_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Median_pre MAE: 0.07500000000000007\n"
    }
   ],
   "source": [
    "Median_pre = Median_method(test_pre1,test_pre2,test_pre3)\n",
    "print('Median_pre MAE:',metrics.mean_absolute_error(y_test_true, Median_pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2)Stacking融合（回归）\n",
    "from sklearn import linear_model\n",
    "\n",
    "def Stacking_method(train_reg1, train_reg2, train_reg3, y_train_true, test_pre1, test_pre2, test_pre3, model_L2=linear_model.LinearRegression()):\n",
    "    model_L2.fit(pd.concat([pd.Series(train_reg1),pd.Series(train_reg2),pd.Series(train_reg3)],axis=1).values, y_train_true)\n",
    "    Stacking_result = model_L2.predict(pd.concat([pd.Series(test_pre1),pd.Series(test_pre2),pd.Series(test_pre3)],axis=1).values)\n",
    "    return Stacking_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成一些简单的样本数据，test_prei 代表第i个模型的预测值\n",
    "train_reg1 = [3.2, 8.2, 9.1, 5.2]\n",
    "train_reg2 = [2.9, 8.1, 9.0, 4.9]\n",
    "train_reg3 = [3.1, 7.9, 9.2, 5.0]\n",
    "# y_test_true 代表第模型的真实值\n",
    "y_train_true = [3, 8, 9, 5] \n",
    "\n",
    "test_pre1 = [1.2, 3.2, 2.1, 6.2]\n",
    "test_pre2 = [0.9, 3.1, 2.0, 5.9]\n",
    "test_pre3 = [1.1, 2.9, 2.2, 6.0]\n",
    "\n",
    "# y_test_true 代表第模型的真实值\n",
    "y_test_true = [1, 3, 2, 6] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Stacking_pre MAE: 0.042134831460675204\n"
    }
   ],
   "source": [
    "model_L2= linear_model.LinearRegression()\n",
    "Stacking_pre = Stacking_method(train_reg1,train_reg2,train_reg3,y_train_true,test_pre1,test_pre2,test_pre3,model_L2)\n",
    "print('Stacking_pre MAE:',metrics.mean_absolute_error(y_test_true, Stacking_pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分类模型融合\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy: 0.96 (+/- 0.02) [XGBBoosting]\nAccuracy: 0.33 (+/- 0.00) [Random Forest]\nAccuracy: 0.92 (+/- 0.03) [SVM]\nAccuracy: 0.92 (+/- 0.03) [Ensemble]\n"
    }
   ],
   "source": [
    "# 1)Voting投票机制\n",
    "'''\n",
    "硬投票：对多个模型直接进行投票，不区分模型结果的相对重要度，最终投票数最多的类为最终被预测的类。\n",
    "'''\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "x=iris.data\n",
    "y=iris.target\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3)\n",
    "\n",
    "clf1 = XGBClassifier(learning_rate=0.1, n_estimators=150, max_depth=3,min_child_weight=2, subsample=0.7,colsample_bytree=0.6,objective='binary:logistic')\n",
    "clf2 = RandomForestClassifier(n_estimators=50, max_depth=1, min_samples_split=4,min_samples_leaf=63,oob_score=True)\n",
    "clf3 = SVC(C=0.1)\n",
    "\n",
    "# 硬投票\n",
    "eclf = VotingClassifier(estimators=[('xgb', clf1), ('rf', clf2), ('svc', clf3)], voting='hard')\n",
    "for clf, label in zip([clf1, clf2, clf3, eclf], ['XGBBoosting', 'Random Forest', 'SVM', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, x, y, cv=5, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy: 0.96 (+/- 0.02) [XGBBoosting]\nAccuracy: 0.33 (+/- 0.00) [Random Forest]\nAccuracy: 0.92 (+/- 0.03) [SVM]\nAccuracy: 0.96 (+/- 0.02) [Ensemble]\n"
    }
   ],
   "source": [
    "'''\n",
    "软投票：和硬投票原理相同，增加了设置权重的功能，可以为不同模型设置不同权重，进而区别模型不同的重要度。\n",
    "'''\n",
    "x=iris.data\n",
    "y=iris.target\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3)\n",
    "\n",
    "clf1 = XGBClassifier(learning_rate=0.1, n_estimators=150, max_depth=3, min_child_weight=2, subsample=0.8,colsample_bytree=0.8, objective='binary:logistic')\n",
    "clf2 = RandomForestClassifier(n_estimators=50, max_depth=1, min_samples_split=4,min_samples_leaf=63,oob_score=True)\n",
    "clf3 = SVC(C=0.1, probability=True)\n",
    "\n",
    "# 软投票\n",
    "eclf = VotingClassifier(estimators=[('xgb', clf1), ('rf', clf2), ('svc', clf3)], voting='soft', weights=[2, 1, 1])\n",
    "clf1.fit(x_train, y_train)\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, eclf], ['XGBBoosting', 'Random Forest', 'SVM', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, x, y, cv=5, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "val auc Score: 1.000000\nval auc Score: 0.500000\nval auc Score: 0.500000\nval auc Score: 0.500000\nval auc Score: 0.500000\nVal auc Score of Stacking: 1.000000\n"
    }
   ],
   "source": [
    "# 2)分类的Stacking/Blending融合\n",
    "'''\n",
    "5-Fold Stacking\n",
    "'''\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier,GradientBoostingClassifier\n",
    "import pandas as pd\n",
    "#创建训练的数据集\n",
    "data_0 = iris.data\n",
    "data = data_0[:100,:]\n",
    "\n",
    "target_0 = iris.target\n",
    "target = target_0[:100]\n",
    "\n",
    "#模型融合中使用到的各个单模型\n",
    "clfs = [LogisticRegression(solver='lbfgs'),\n",
    "        RandomForestClassifier(n_estimators=5, n_jobs=-1, criterion='gini'),\n",
    "        ExtraTreesClassifier(n_estimators=5, n_jobs=-1, criterion='gini'),\n",
    "        ExtraTreesClassifier(n_estimators=5, n_jobs=-1, criterion='entropy'),\n",
    "        GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=5)]\n",
    " \n",
    "#切分一部分数据作为测试集\n",
    "X, X_predict, y, y_predict = train_test_split(data, target, test_size=0.3, random_state=2020)\n",
    "\n",
    "dataset_blend_train = np.zeros((X.shape[0], len(clfs)))\n",
    "dataset_blend_test = np.zeros((X_predict.shape[0], len(clfs)))\n",
    "\n",
    "#5折stacking\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits)\n",
    "skf = skf.split(X, y)\n",
    "\n",
    "for j, clf in enumerate(clfs):\n",
    "    #依次训练各个单模型\n",
    "    dataset_blend_test_j = np.zeros((X_predict.shape[0], 5))\n",
    "    for i, (train, test) in enumerate(skf):\n",
    "        #5-Fold交叉训练，使用第i个部分作为预测，剩余的部分来训练模型，获得其预测的输出作为第i部分的新特征。\n",
    "        X_train, y_train, X_test, y_test = X[train], y[train], X[test], y[test]\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_submission = clf.predict_proba(X_test)[:, 1]\n",
    "        dataset_blend_train[test, j] = y_submission\n",
    "        dataset_blend_test_j[:, i] = clf.predict_proba(X_predict)[:, 1]\n",
    "    #对于测试集，直接用这k个模型的预测值均值作为新的特征。\n",
    "    dataset_blend_test[:, j] = dataset_blend_test_j.mean(1)\n",
    "    print(\"val auc Score: %f\" % roc_auc_score(y_predict, dataset_blend_test[:, j]))\n",
    "\n",
    "clf = LogisticRegression(solver='lbfgs')\n",
    "clf.fit(dataset_blend_train, y)\n",
    "y_submission = clf.predict_proba(dataset_blend_test)[:, 1]\n",
    "\n",
    "print(\"Val auc Score of Stacking: %f\" % (roc_auc_score(y_predict, y_submission)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "val auc Score: 1.000000\nval auc Score: 1.000000\nval auc Score: 1.000000\nval auc Score: 1.000000\nval auc Score: 1.000000\nVal auc Score of Blending: 1.000000\n"
    }
   ],
   "source": [
    "'''\n",
    "Blending\n",
    "'''\n",
    "\n",
    "#创建训练的数据集\n",
    "data_0 = iris.data\n",
    "data = data_0[:100,:]\n",
    "\n",
    "target_0 = iris.target\n",
    "target = target_0[:100]\n",
    " \n",
    "#模型融合中使用到的各个单模型\n",
    "clfs = [LogisticRegression(solver='lbfgs'),\n",
    "        RandomForestClassifier(n_estimators=5, n_jobs=-1, criterion='gini'),\n",
    "        RandomForestClassifier(n_estimators=5, n_jobs=-1, criterion='entropy'),\n",
    "        ExtraTreesClassifier(n_estimators=5, n_jobs=-1, criterion='gini'),\n",
    "        #ExtraTreesClassifier(n_estimators=5, n_jobs=-1, criterion='entropy'),\n",
    "        GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=5)]\n",
    " \n",
    "#切分一部分数据作为测试集\n",
    "X, X_predict, y, y_predict = train_test_split(data, target, test_size=0.3, random_state=2020)\n",
    "\n",
    "#切分训练数据集为d1,d2两部分\n",
    "X_d1, X_d2, y_d1, y_d2 = train_test_split(X, y, test_size=0.5, random_state=2020)\n",
    "dataset_d1 = np.zeros((X_d2.shape[0], len(clfs)))\n",
    "dataset_d2 = np.zeros((X_predict.shape[0], len(clfs)))\n",
    " \n",
    "for j, clf in enumerate(clfs):\n",
    "    #依次训练各个单模型\n",
    "    clf.fit(X_d1, y_d1)\n",
    "    y_submission = clf.predict_proba(X_d2)[:, 1]\n",
    "    dataset_d1[:, j] = y_submission\n",
    "    #对于测试集，直接用这k个模型的预测值作为新的特征。\n",
    "    dataset_d2[:, j] = clf.predict_proba(X_predict)[:, 1]\n",
    "    print(\"val auc Score: %f\" % roc_auc_score(y_predict, dataset_d2[:, j]))\n",
    "\n",
    "#融合使用的模型\n",
    "clf = GradientBoostingClassifier(learning_rate=0.02, subsample=0.5, max_depth=6, n_estimators=30)\n",
    "clf.fit(dataset_d1, y_d2)\n",
    "y_submission = clf.predict_proba(dataset_d2)[:, 1]\n",
    "print(\"Val auc Score of Blending: %f\" % (roc_auc_score(y_predict, y_submission)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Requirement already satisfied: mlxtend in d:\\anaconda\\lib\\site-packages (0.17.2)\nRequirement already satisfied: setuptools in d:\\anaconda\\lib\\site-packages (from mlxtend) (41.0.1)\nRequirement already satisfied: numpy>=1.16.2 in d:\\anaconda\\lib\\site-packages (from mlxtend) (1.17.4)\nRequirement already satisfied: pandas>=0.24.2 in d:\\anaconda\\lib\\site-packages (from mlxtend) (0.25.3)\nRequirement already satisfied: scikit-learn>=0.20.3 in d:\\anaconda\\lib\\site-packages (from mlxtend) (0.22)\nRequirement already satisfied: matplotlib>=3.0.0 in d:\\anaconda\\lib\\site-packages (from mlxtend) (3.1.2)\nRequirement already satisfied: scipy>=1.2.1 in d:\\anaconda\\lib\\site-packages (from mlxtend) (1.4.1)\nRequirement already satisfied: joblib>=0.13.2 in d:\\anaconda\\lib\\site-packages (from mlxtend) (0.14.1)\nRequirement already satisfied: python-dateutil>=2.6.1 in d:\\anaconda\\lib\\site-packages (from pandas>=0.24.2->mlxtend) (2.8.0)\nRequirement already satisfied: pytz>=2017.2 in d:\\anaconda\\lib\\site-packages (from pandas>=0.24.2->mlxtend) (2019.1)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in d:\\anaconda\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (2.4.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in d:\\anaconda\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.1.0)\nRequirement already satisfied: cycler>=0.10 in d:\\anaconda\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (0.10.0)\nRequirement already satisfied: six>=1.5 in d:\\anaconda\\lib\\site-packages (from python-dateutil>=2.6.1->pandas>=0.24.2->mlxtend) (1.12.0)\nAccuracy: 0.91 (+/- 0.01) [KNN]\nAccuracy: 0.95 (+/- 0.01) [Random Forest]\nAccuracy: 0.91 (+/- 0.02) [Naive Bayes]\nAccuracy: 0.95 (+/- 0.02) [Stacking Classifier]\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 1000x800 with 4 Axes>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# 3) 分类的Stacking融合（利用mlxtend)\n",
    "\n",
    "!pip install mlxtend\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import itertools\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from mlxtend.plotting import plot_learning_curves\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "# 以python自带的鸢尾花数据集为例\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[:, 1:3], iris.target\n",
    "\n",
    "clf1 = KNeighborsClassifier(n_neighbors=1)\n",
    "clf2 = RandomForestClassifier(random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "lr = LogisticRegression()\n",
    "sclf = StackingClassifier(classifiers=[clf1, clf2, clf3], \n",
    "                          meta_classifier=lr)\n",
    "\n",
    "label = ['KNN', 'Random Forest', 'Naive Bayes', 'Stacking Classifier']\n",
    "clf_list = [clf1, clf2, clf3, sclf]\n",
    "\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "grid = itertools.product([0,1],repeat=2)\n",
    "\n",
    "clf_cv_mean = []\n",
    "clf_cv_std = []\n",
    "for clf, label, grd in zip(clf_list, label, grid):\n",
    "        \n",
    "    scores = cross_val_score(clf, X, y, cv=3, scoring='accuracy')\n",
    "    print(\"Accuracy: %.2f (+/- %.2f) [%s]\" %(scores.mean(), scores.std(), label))\n",
    "    clf_cv_mean.append(scores.mean())\n",
    "    clf_cv_std.append(scores.std())\n",
    "        \n",
    "    clf.fit(X, y)\n",
    "    ax = plt.subplot(gs[grd[0], grd[1]])\n",
    "    fig = plot_decision_regions(X=X, y=y, clf=clf)\n",
    "    plt.title(label)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#一些其他的方法\n",
    "def Ensemble_add_feature(train,test,target,clfs):\n",
    "    \n",
    "    # n_flods = 5\n",
    "    # skf = list(StratifiedKFold(y, n_folds=n_flods))\n",
    "\n",
    "    train_ = np.zeros((train.shape[0],len(clfs*2)))\n",
    "    test_ = np.zeros((test.shape[0],len(clfs*2)))\n",
    "\n",
    "    for j,clf in enumerate(clfs):\n",
    "        '''依次训练各个单模型'''\n",
    "        # print(j, clf)\n",
    "        '''使用第1个部分作为预测，第2部分来训练模型，获得其预测的输出作为第2部分的新特征。'''\n",
    "        # X_train, y_train, X_test, y_test = X[train], y[train], X[test], y[test]\n",
    "\n",
    "        clf.fit(train,target)\n",
    "        y_train = clf.predict(train)\n",
    "        y_test = clf.predict(test)\n",
    "\n",
    "        ## 新特征生成\n",
    "        train_[:,j*2] = y_train**2\n",
    "        test_[:,j*2] = y_test**2\n",
    "        train_[:, j+1] = np.exp(y_train)\n",
    "        test_[:, j+1] = np.exp(y_test)\n",
    "        # print(\"val auc Score: %f\" % r2_score(y_predict, dataset_d2[:, j]))\n",
    "        print('Method ',j)\n",
    "    \n",
    "    train_ = pd.DataFrame(train_)\n",
    "    test_ = pd.DataFrame(test_)\n",
    "    return train_,test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Method  0\nMethod  1\nMethod  2\nMethod  3\nMethod  4\nVal auc Score of stacking: 1.000000\n"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "\n",
    "data_0 = iris.data\n",
    "data = data_0[:100,:]\n",
    "\n",
    "target_0 = iris.target\n",
    "target = target_0[:100]\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(data,target,test_size=0.3)\n",
    "x_train = pd.DataFrame(x_train) ; x_test = pd.DataFrame(x_test)\n",
    "\n",
    "#模型融合中使用到的各个单模型\n",
    "clfs = [LogisticRegression(),\n",
    "        RandomForestClassifier(n_estimators=5, n_jobs=-1, criterion='gini'),\n",
    "        ExtraTreesClassifier(n_estimators=5, n_jobs=-1, criterion='gini'),\n",
    "        ExtraTreesClassifier(n_estimators=5, n_jobs=-1, criterion='entropy'),\n",
    "        GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=5)]\n",
    "\n",
    "New_train,New_test = Ensemble_add_feature(x_train,x_test,y_train,clfs)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "# clf = GradientBoostingClassifier(learning_rate=0.02, subsample=0.5, max_depth=6, n_estimators=30)\n",
    "clf.fit(New_train, y_train)\n",
    "y_emb = clf.predict_proba(New_test)[:, 1]\n",
    "\n",
    "print(\"Val auc Score of stacking: %f\" % (roc_auc_score(y_test, y_emb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本赛题示例\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "# from mlxtend.plotting import plot_learning_curves\n",
    "# from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.decomposition import PCA,FastICA,FactorAnalysis,SparsePCA\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV,cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(150000, 31)\n(50000, 30)\n"
    }
   ],
   "source": [
    "Train_data = pd.read_csv('E:/Git-repository/data_mining/二手车价格预测组队学习/data/used_car_train_20200313.csv', sep=' ')\n",
    "TestA_data = pd.read_csv('E:/Git-repository/data_mining/二手车价格预测组队学习/data/used_car_testA_20200313.csv', sep=' ')\n",
    "\n",
    "print(Train_data.shape)\n",
    "print(TestA_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   SaleID    name   regDate  model  brand  bodyType  fuelType  gearbox  power  \\\n0       0     736  20040402   30.0      6       1.0       0.0      0.0     60   \n1       1    2262  20030301   40.0      1       2.0       0.0      0.0      0   \n2       2   14874  20040403  115.0     15       1.0       0.0      0.0    163   \n3       3   71865  19960908  109.0     10       0.0       0.0      1.0    193   \n4       4  111080  20120103  110.0      5       1.0       0.0      0.0     68   \n\n   kilometer  ...       v_5       v_6       v_7       v_8       v_9      v_10  \\\n0       12.5  ...  0.235676  0.101988  0.129549  0.022816  0.097462 -2.881803   \n1       15.0  ...  0.264777  0.121004  0.135731  0.026597  0.020582 -4.900482   \n2       12.5  ...  0.251410  0.114912  0.165147  0.062173  0.027075 -4.846749   \n3       15.0  ...  0.274293  0.110300  0.121964  0.033395  0.000000 -4.509599   \n4        5.0  ...  0.228036  0.073205  0.091880  0.078819  0.121534 -1.896240   \n\n       v_11      v_12      v_13      v_14  \n0  2.804097 -2.420821  0.795292  0.914762  \n1  2.096338 -1.030483 -1.722674  0.245522  \n2  1.803559  1.565330 -0.832687 -0.229963  \n3  1.285940 -0.501868 -2.438353 -0.478699  \n4  0.910783  0.931110  2.834518  1.923482  \n\n[5 rows x 31 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SaleID</th>\n      <th>name</th>\n      <th>regDate</th>\n      <th>model</th>\n      <th>brand</th>\n      <th>bodyType</th>\n      <th>fuelType</th>\n      <th>gearbox</th>\n      <th>power</th>\n      <th>kilometer</th>\n      <th>...</th>\n      <th>v_5</th>\n      <th>v_6</th>\n      <th>v_7</th>\n      <th>v_8</th>\n      <th>v_9</th>\n      <th>v_10</th>\n      <th>v_11</th>\n      <th>v_12</th>\n      <th>v_13</th>\n      <th>v_14</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>736</td>\n      <td>20040402</td>\n      <td>30.0</td>\n      <td>6</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>60</td>\n      <td>12.5</td>\n      <td>...</td>\n      <td>0.235676</td>\n      <td>0.101988</td>\n      <td>0.129549</td>\n      <td>0.022816</td>\n      <td>0.097462</td>\n      <td>-2.881803</td>\n      <td>2.804097</td>\n      <td>-2.420821</td>\n      <td>0.795292</td>\n      <td>0.914762</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2262</td>\n      <td>20030301</td>\n      <td>40.0</td>\n      <td>1</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>15.0</td>\n      <td>...</td>\n      <td>0.264777</td>\n      <td>0.121004</td>\n      <td>0.135731</td>\n      <td>0.026597</td>\n      <td>0.020582</td>\n      <td>-4.900482</td>\n      <td>2.096338</td>\n      <td>-1.030483</td>\n      <td>-1.722674</td>\n      <td>0.245522</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>14874</td>\n      <td>20040403</td>\n      <td>115.0</td>\n      <td>15</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>163</td>\n      <td>12.5</td>\n      <td>...</td>\n      <td>0.251410</td>\n      <td>0.114912</td>\n      <td>0.165147</td>\n      <td>0.062173</td>\n      <td>0.027075</td>\n      <td>-4.846749</td>\n      <td>1.803559</td>\n      <td>1.565330</td>\n      <td>-0.832687</td>\n      <td>-0.229963</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>71865</td>\n      <td>19960908</td>\n      <td>109.0</td>\n      <td>10</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>193</td>\n      <td>15.0</td>\n      <td>...</td>\n      <td>0.274293</td>\n      <td>0.110300</td>\n      <td>0.121964</td>\n      <td>0.033395</td>\n      <td>0.000000</td>\n      <td>-4.509599</td>\n      <td>1.285940</td>\n      <td>-0.501868</td>\n      <td>-2.438353</td>\n      <td>-0.478699</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>111080</td>\n      <td>20120103</td>\n      <td>110.0</td>\n      <td>5</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>68</td>\n      <td>5.0</td>\n      <td>...</td>\n      <td>0.228036</td>\n      <td>0.073205</td>\n      <td>0.091880</td>\n      <td>0.078819</td>\n      <td>0.121534</td>\n      <td>-1.896240</td>\n      <td>0.910783</td>\n      <td>0.931110</td>\n      <td>2.834518</td>\n      <td>1.923482</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 31 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "Train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Index(['SaleID', 'name', 'regDate', 'model', 'brand', 'bodyType', 'fuelType',\n       'gearbox', 'power', 'kilometer', 'regionCode', 'seller', 'offerType',\n       'creatDate', 'price', 'v_0', 'v_1', 'v_2', 'v_3', 'v_4', 'v_5', 'v_6',\n       'v_7', 'v_8', 'v_9', 'v_10', 'v_11', 'v_12', 'v_13', 'v_14'],\n      dtype='object')\n"
    }
   ],
   "source": [
    "numerical_cols = Train_data.select_dtypes(exclude = 'object').columns\n",
    "print(numerical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [col for col in numerical_cols if col not in ['SaleID','name','regDate','price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "X train shape: (150000, 26)\nX test shape: (50000, 26)\n"
    }
   ],
   "source": [
    "X_data = Train_data[feature_cols]\n",
    "Y_data = Train_data['price']\n",
    "\n",
    "X_test  = TestA_data[feature_cols]\n",
    "\n",
    "print('X train shape:',X_data.shape)\n",
    "print('X test shape:',X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sta_inf(data):\n",
    "    print('_min',np.min(data))\n",
    "    print('_max:',np.max(data))\n",
    "    print('_mean',np.mean(data))\n",
    "    print('_ptp',np.ptp(data))\n",
    "    print('_std',np.std(data))\n",
    "    print('_var',np.var(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Sta of label:\n_min 11\n_max: 99999\n_mean 5923.327333333334\n_ptp 99988\n_std 7501.973469876438\n_var 56279605.94272992\n"
    }
   ],
   "source": [
    "print('Sta of label:')\n",
    "Sta_inf(Y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = X_data.fillna(-1)\n",
    "X_test = X_test.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_lr(x_train,y_train):\n",
    "    reg_model = linear_model.LinearRegression()\n",
    "    reg_model.fit(x_train,y_train)\n",
    "    return reg_model\n",
    "\n",
    "def build_model_ridge(x_train,y_train):\n",
    "    reg_model = linear_model.Ridge(alpha=0.8)#alphas=range(1,100,5)\n",
    "    reg_model.fit(x_train,y_train)\n",
    "    return reg_model\n",
    "\n",
    "def build_model_lasso(x_train,y_train):\n",
    "    reg_model = linear_model.LassoCV()\n",
    "    reg_model.fit(x_train,y_train)\n",
    "    return reg_model\n",
    "\n",
    "def build_model_gbdt(x_train,y_train):\n",
    "    estimator =GradientBoostingRegressor(loss='ls',subsample= 0.85,max_depth= 5,n_estimators = 100)\n",
    "    param_grid = { \n",
    "            'learning_rate': [0.05,0.08,0.1,0.2],\n",
    "            }\n",
    "    gbdt = GridSearchCV(estimator, param_grid,cv=3)\n",
    "    gbdt.fit(x_train,y_train)\n",
    "    print(gbdt.best_params_)\n",
    "    # print(gbdt.best_estimator_ )\n",
    "    return gbdt\n",
    "\n",
    "def build_model_xgb(x_train,y_train):\n",
    "    model = xgb.XGBRegressor(n_estimators=120, learning_rate=0.08, gamma=0, subsample=0.8,\\\n",
    "        colsample_bytree=0.9, max_depth=5) #, objective ='reg:squarederror'\n",
    "    model.fit(x_train, y_train)\n",
    "    return model\n",
    "\n",
    "def build_model_lgb(x_train,y_train):\n",
    "    estimator = lgb.LGBMRegressor(num_leaves=63,n_estimators = 100)\n",
    "    param_grid = {\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "    }\n",
    "    gbm = GridSearchCV(estimator, param_grid)\n",
    "    gbm.fit(x_train, y_train)\n",
    "    return gbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train mae: 594.8909340395033\nVal mae 693.382067947197\n"
    }
   ],
   "source": [
    "# 2)XGBoost的五折交叉回归验证实现\n",
    "\n",
    "## xgb\n",
    "xgr = xgb.XGBRegressor(n_estimators=120, learning_rate=0.1, subsample=0.8,\\\n",
    "        colsample_bytree=0.9, max_depth=7) # ,objective ='reg:squarederror'\n",
    "\n",
    "scores_train = []\n",
    "scores = []\n",
    "\n",
    "## 5折交叉验证方式\n",
    "sk=StratifiedKFold(n_splits=5,shuffle=True,random_state=0)\n",
    "for train_ind,val_ind in sk.split(X_data,Y_data):\n",
    "    \n",
    "    train_x=X_data.iloc[train_ind].values\n",
    "    train_y=Y_data.iloc[train_ind]\n",
    "    val_x=X_data.iloc[val_ind].values\n",
    "    val_y=Y_data.iloc[val_ind]\n",
    "    \n",
    "    xgr.fit(train_x,train_y)\n",
    "    pred_train_xgb=xgr.predict(train_x)\n",
    "    pred_xgb=xgr.predict(val_x)\n",
    "    \n",
    "    score_train = mean_absolute_error(train_y,pred_train_xgb)\n",
    "    scores_train.append(score_train)\n",
    "    score = mean_absolute_error(val_y,pred_xgb)\n",
    "    scores.append(score)\n",
    "\n",
    "print('Train mae:',np.mean(score_train))\n",
    "print('Val mae',np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Predict LR...\nPredict Ridge...\nPredict Lasso...\nPredict GBDT...\n{'learning_rate': 0.2}\n"
    }
   ],
   "source": [
    "# 3）划分数据集，并用多种方法训练和预测\n",
    "\n",
    "## Split data with val\n",
    "x_train,x_val,y_train,y_val = train_test_split(X_data,Y_data,test_size=0.3)\n",
    "\n",
    "## Train and Predict\n",
    "print('Predict LR...')\n",
    "model_lr = build_model_lr(x_train,y_train)\n",
    "val_lr = model_lr.predict(x_val)\n",
    "subA_lr = model_lr.predict(X_test)\n",
    "\n",
    "print('Predict Ridge...')\n",
    "model_ridge = build_model_ridge(x_train,y_train)\n",
    "val_ridge = model_ridge.predict(x_val)\n",
    "subA_ridge = model_ridge.predict(X_test)\n",
    "\n",
    "print('Predict Lasso...')\n",
    "model_lasso = build_model_lasso(x_train,y_train)\n",
    "val_lasso = model_lasso.predict(x_val)\n",
    "subA_lasso = model_lasso.predict(X_test)\n",
    "\n",
    "print('Predict GBDT...')\n",
    "model_gbdt = build_model_gbdt(x_train,y_train)\n",
    "val_gbdt = model_gbdt.predict(x_val)\n",
    "subA_gbdt = model_gbdt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "predict XGB...\n"
    }
   ],
   "source": [
    "# 一般比赛中效果最为显著的两种方法\n",
    "# (1)\n",
    "print('predict XGB...')\n",
    "model_xgb = build_model_xgb(x_train,y_train)\n",
    "val_xgb = model_xgb.predict(x_val)\n",
    "subA_xgb = model_xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "predict lgb...\n"
    }
   ],
   "source": [
    "# (2)\n",
    "print('predict lgb...')\n",
    "model_lgb = build_model_lgb(x_train,y_train)\n",
    "val_lgb = model_lgb.predict(x_val)\n",
    "subA_lgb = model_lgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Sta inf of lgb:\n_min -202.38060813004992\n_max: 87511.60581859411\n_mean 5923.7601176327835\n_ptp 87713.98642672415\n_std 7356.05620587724\n_var 54111562.904025055\n"
    }
   ],
   "source": [
    "print('Sta inf of lgb:')\n",
    "Sta_inf(subA_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "MAE of Weighted of val: 729.5466648760192\nSta inf:\n_min -150.43171275927418\n_max: 89003.12238323814\n_mean 5926.332188502673\n_ptp 89153.55409599742\n_std 7342.418035045434\n_var 53911102.60136046\n"
    }
   ],
   "source": [
    "# 1）加权融合\n",
    "def Weighted_method(test_pre1,test_pre2,test_pre3,w=[1/3,1/3,1/3]):\n",
    "    Weighted_result = w[0]*pd.Series(test_pre1)+w[1]*pd.Series(test_pre2)+w[2]*pd.Series(test_pre3)\n",
    "    return Weighted_result\n",
    "\n",
    "## Init the Weight\n",
    "w = [0.3,0.4,0.3]\n",
    "\n",
    "## 测试验证集准确度\n",
    "val_pre = Weighted_method(val_lgb,val_xgb,val_gbdt,w)\n",
    "MAE_Weighted = mean_absolute_error(y_val,val_pre)\n",
    "print('MAE of Weighted of val:',MAE_Weighted)\n",
    "\n",
    "## 预测数据部分\n",
    "subA = Weighted_method(subA_lgb,subA_xgb,subA_gbdt,w)\n",
    "print('Sta inf:')\n",
    "Sta_inf(subA)\n",
    "## 生成提交文件\n",
    "sub = pd.DataFrame()\n",
    "sub['SaleID'] = X_test.index\n",
    "sub['price'] = subA\n",
    "sub.to_csv('./sub_Weighted.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "MAE of lr: 2595.9058259998324\n"
    }
   ],
   "source": [
    "## 与简单的LR（线性回归）进行对比\n",
    "val_lr_pred = model_lr.predict(x_val)\n",
    "MAE_lr = mean_absolute_error(y_val,val_lr_pred)\n",
    "print('MAE of lr:',MAE_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2）Starking融合\n",
    "\n",
    "## Starking\n",
    "\n",
    "## 第一层\n",
    "train_lgb_pred = model_lgb.predict(x_train)\n",
    "train_xgb_pred = model_xgb.predict(x_train)\n",
    "train_gbdt_pred = model_gbdt.predict(x_train)\n",
    "\n",
    "Strak_X_train = pd.DataFrame()\n",
    "Strak_X_train['Method_1'] = train_lgb_pred\n",
    "Strak_X_train['Method_2'] = train_xgb_pred\n",
    "Strak_X_train['Method_3'] = train_gbdt_pred\n",
    "\n",
    "Strak_X_val = pd.DataFrame()\n",
    "Strak_X_val['Method_1'] = val_lgb\n",
    "Strak_X_val['Method_2'] = val_xgb\n",
    "Strak_X_val['Method_3'] = val_gbdt\n",
    "\n",
    "Strak_X_test = pd.DataFrame()\n",
    "Strak_X_test['Method_1'] = subA_lgb\n",
    "Strak_X_test['Method_2'] = subA_xgb\n",
    "Strak_X_test['Method_3'] = subA_gbdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "       Method_1      Method_2      Method_3\n0  37799.165961  37117.312500  38574.842481\n1    322.049898    350.673889    251.686390\n2   6857.928255   7460.464844   6907.367520\n3  11439.823258  11572.526367  11678.249386\n4    570.096995    536.420288    578.198798",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Method_1</th>\n      <th>Method_2</th>\n      <th>Method_3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>37799.165961</td>\n      <td>37117.312500</td>\n      <td>38574.842481</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>322.049898</td>\n      <td>350.673889</td>\n      <td>251.686390</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>6857.928255</td>\n      <td>7460.464844</td>\n      <td>6907.367520</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11439.823258</td>\n      <td>11572.526367</td>\n      <td>11678.249386</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>570.096995</td>\n      <td>536.420288</td>\n      <td>578.198798</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "Strak_X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "MAE of Stacking-LR: 630.085615711908\nMAE of Stacking-LR: 717.9386727025048\nPredict Stacking-LR...\n"
    }
   ],
   "source": [
    "## level2-method \n",
    "model_lr_Stacking = build_model_lr(Strak_X_train,y_train)\n",
    "## 训练集\n",
    "train_pre_Stacking = model_lr_Stacking.predict(Strak_X_train)\n",
    "print('MAE of Stacking-LR:',mean_absolute_error(y_train,train_pre_Stacking))\n",
    "\n",
    "## 验证集\n",
    "val_pre_Stacking = model_lr_Stacking.predict(Strak_X_val)\n",
    "print('MAE of Stacking-LR:',mean_absolute_error(y_val,val_pre_Stacking))\n",
    "\n",
    "## 预测集\n",
    "print('Predict Stacking-LR...')\n",
    "subA_Stacking = model_lr_Stacking.predict(Strak_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "subA_Stacking[subA_Stacking<10]=10  ## 去除过小的预测值\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['SaleID'] = X_test.index\n",
    "sub['price'] = subA_Stacking\n",
    "sub.to_csv('./sub_Stacking.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Sta inf:\n_min 10.0\n_max: 89988.65532406767\n_mean 5924.591361057722\n_ptp 89978.65532406767\n_std 7392.63100892809\n_var 54650993.23416514\n"
    }
   ],
   "source": [
    "print('Sta inf:')\n",
    "Sta_inf(subA_Stacking)"
   ]
  }
 ]
}